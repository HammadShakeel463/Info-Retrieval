1. Flat clustering creates a flat set of clusters without any explicit structure that would relate clusters to each other. Hierarchical clustering creates a hierarchy of clusters

2. A second important distinction can be made between hard and soft clustering algorithms. Hard clustering computes a hard assignment – each document is a member of exactly one cluster. The assignment of soft clustering algorithms is soft – a document’s assignment is a distribution over all clusters. In a soft assignment, a document has fractional membership in several clusters. Latent semantic indexing, a form of dimensionality reduction, is a soft clustering algorithm

3. Cluster hypothesis. Documents in the same cluster behave similarly with respect to relevance to information needs.

4. Partitional clustering always refers to a clustering where each document belongs to exactly one cluster. (But in a partitional hierarchical clustering all members of a cluster are of course also members of its parent.)

5. exhaustive clusterings that assign each document to a cluster and non-exhaustive clusterings, in which some which each document is a member of either no cluster or one cluster are called exclusive.

6. A difficult issue in clustering is determining the number of clusters or cardinality of a clustering, which we denote by K

7. To compute purity, each cluster is assigned to the class which is most frequent in the cluster, and then the accuracy of this assignment is measured by counting the number of correctly assigned documents and dividing by N.

8. The Rand index (RI) measures the percentage of decisions that are correct. That is, it is simply accuracy

9. A measure of how well the centroids represent the members of their clusters is the residual sum of squares or RSS, the squared distance of each vector from its centroid summed over all vectors.

10. The first step of K-means is to select as initial cluster centers K randomly selected documents, the seeds.

11. singleton cluster (a cluster with only one document)

12. The same efficiency problem is addressed by K-medoids, a variant of Kmeans that computes medoids instead of centroids as cluster centers. We define the medoid of a cluster as the document vector that is closest to the centroid.

12. To determine the cluster cardinality in this way, we create a generalized objective function that combines two elements: distortion, a measure of how much documents deviate from the prototype of their clusters (e.g., RSS for K-means); and a measure of model complexity

13. Model-based clustering assumes that the data were generated by a model and tries to recover the original model from the data.

14.  commonly used algorithm for model-based clustering is the Maximization algorithm or EM algorithm. EM clustering is an iterative algorithm that maximizes L(D|Θ)

15. 





